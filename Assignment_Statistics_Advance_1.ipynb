{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistics Advance-1 Assignment Questions and Answers**"
      ],
      "metadata": {
        "id": "r5JlpFErGCkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 :- Explain the properties of the F-distribution?"
      ],
      "metadata": {
        "id": "S8ho4_8IGa0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### The F-distribution :- It is a probability  distribution that is useful in the context of comparing variance of two or more sample. particularly in analysis of variance (ANOVA) tests.\n",
        "\n",
        "### The properties of the F-distribution are :-\n",
        "\n",
        "- 1) **The shape of F-distribution is positively skewed,** meaning it has a long tail on the right side. As the degrees of freedom increase, the distribution becomes more symmetric.\n",
        "\n",
        "- 2) **It takes only non-negative values** means the F-distribution start from 0 and go to infinity.\n",
        "\n",
        "- 3) **It is defined by two sets of degrees of freedom** one for the numerator (related to the variance being tested) and one for the denominator (related to the variance of the error).\n",
        "\n",
        "- 4) The Mean of the F-distribution is greater than 1 when both degrees of freedom are greater than 2. If either is less than or equal to 2, the mean is undefined.\n",
        "\n",
        "- 5) The variance of the F-distribution can be calculated, but it's more complex. Generally, it's larger than the mean and depends on the degrees of freedom.\n",
        "\n",
        "- 6) The critical values of the F-distribution are found using F-table, and these are used to decide if the test statistic indicates a significant result.\n"
      ],
      "metadata": {
        "id": "WAv5I6LtGaxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 :- In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "Eg5RcZPEGauc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### The F-distribution is used in several types of statistical tests, mainly because it helps compare variances across groups.\n",
        "\n",
        "### It is appropriate for these tests :-\n",
        "\n",
        "- 1) **ANOVA (Analysis of Variance) :-** This test compares the means of three or more groups to see if at least one group differs significantly. The F-distribution is used because it evaluates the ratio of variances between groups and within groups.\n",
        "\n",
        "- 2) **Regression Analysis :-** In multiple regression, the F-test assesses whether the model as a whole is statistically significant by comparing the variance explained by the model to the variance not explained by the model.\n",
        "\n",
        "- 3) **Comparing Two Variances :-** The F-test can directly compare the variances of two samples to determine if they are significantly different.\n",
        "\n",
        "###  is it appropriate because of the followings :-\n",
        "\n",
        "- 1) **Variance Ratios :-** The F-distribution is based on the ratio of variances, making it ideal for tests that involve variance comparison.\n",
        "\n",
        "- 2) **Positive Skew :-** Its positive skewness fits the nature of variances, which cannot be negative.\n",
        "\n",
        "- 3) **Asymptotic Properties :-** As sample sizes increase, the F-distribution approximates normality, allowing for better statistical inferences."
      ],
      "metadata": {
        "id": "TbYY1ixvGar1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 :- What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
      ],
      "metadata": {
        "id": "bsVZTplcGapH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### The key assumptions required for conducting an F-test to compare the variances of two populations are :-\n",
        "\n",
        "- The samples must be independent of each other. This means that the selection or outcome of one sample should not influence the other.\n",
        "\n",
        "- The populations from which the samples are drawn should follow a normal distribution. If the sample sizes are small, this assumption is particularly important.\n",
        "\n",
        "- The variances of the two populations being compared should be equal (or approximately equal). This is a core assumption of the F-test.\n",
        "\n",
        "- The samples should be randomly selected from the populations to ensure that they are representative."
      ],
      "metadata": {
        "id": "pDkvtf6PGama"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 :- What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "QZhQ8--SGajy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### **ANOVA (Analysis of Variance) :-** ANOVA is used to determine whether there are statistically significant differences between the means of three or more groups.\n",
        "\n",
        "- Generally ANOVA is used for more than 2 groups.\n",
        "\n",
        "- It's primary purpose is to assess whether any of the group differences observed are likely due to random chance or if they are statistically significant.\n",
        "\n",
        "\n",
        "### **ANOVA is differ drom a t-test are below mentioned points :-**\n",
        "\n",
        "- **1) Number of Groups :-**\n",
        "\n",
        "- - **t-test :-** Typically used to compare the means of two groups.\n",
        "\n",
        "- - **ANOVA :-** Used to compare the means of three or more groups.\n",
        "\n",
        "- **2) Hypotheses :-**\n",
        "\n",
        "- - **t-test :-** Tests the null hypothesis that the means of two groups are equal.\n",
        "\n",
        "- - **ANOVA :-** Tests the null hypothesis that all group means are equal. If the ANOVA shows significant results, post-hoc tests are often required to determine which specific groups differ.\n",
        "\n",
        "- **3) Type of Analysis :-**\n",
        "\n",
        "- - **t-test :-** Focuses on pairwise comparisons between groups.\n",
        "\n",
        "- - **ANOVA :-** Looks at variance within and between groups to assess overall differences.\n",
        "\n",
        "- **4) Assumptions :-**\n",
        "Both tests assume normal distribution of data and homogeneity of variance, but ANOVA can handle more complexity regarding group size and variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "3NzaWZ0DGahK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 :- Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
      ],
      "metadata": {
        "id": "RbSXJQW-Gaem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer :-\n",
        "\n",
        "### One-way ANOVA is a statistically sound approach when comparing more than two groups, as it controls error rates, enhances efficiency, and provides a clearer view of the data's variability.\n",
        "\n",
        "### When to Use One-Way ANOVA :-\n",
        "\n",
        "- **When you are Comparing More than Two Groups :-** When you have three or more groups to compare, one-way ANOVA is the appropriate choice.\n",
        "\n",
        "\n",
        "### The Reason Why Use One-Way ANOVA mentioned below :-\n",
        "\n",
        "- **1) Type I Error Control :-** Conducting multiple t-tests increases the risk of committing a Type I error (incorrectly rejecting the null hypothesis). Each t-test carries a probability of error, and with multiple tests, the overall error rate increases. One-way ANOVA maintains a single error rate across all comparisons.\n",
        "\n",
        "- **2) Efficiency :-** ANOVA tests the overall difference among group means in one analysis rather than conducting multiple tests, which is more efficient and easier to interpret.\n",
        "\n",
        "- **3) Variance Analysis :-** ANOVA assesses variance between group means and variance within groups simultaneously. This provides a more comprehensive understanding of the data.\n",
        "\n",
        "- **4) Post-Hoc Analysis :-** If ANOVA indicates significant differences, post-hoc tests (example:- Tukey's HSD) can be applied to identify which specific groups differ, allowing for a focused analysis without the multiple comparison problem.\n",
        "\n",
        "- **5) Assumption Handling :-** One-way ANOVA can handle situations where the group sizes are unequal better than multiple t-tests, which may require adjustments for unequal variances."
      ],
      "metadata": {
        "id": "z2Q3VvvGGabt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic ?"
      ],
      "metadata": {
        "id": "R6-IJ681GaZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### In ANOVA, variance is partitioned into two components :-\n",
        "\n",
        "- **i) between-group variance**\n",
        "- **ii) within-group variance**\n",
        "\n",
        "This partitioning is crucial for understanding how the overall variability in the data is attributed to different sources and plays a key role in calculating the F-statistic.\n",
        "\n",
        "\n",
        "### **1) Between-Group Variance :-**\n",
        "\n",
        "- This component measures the variability in group means relative to the overall mean. It reflects how much the group means differ from each other.\n",
        "\n",
        "- This variance is calculated by taking the squared differences between each group mean and the overall mean, weighted by the number of observations in each group.\n",
        "\n",
        "### **2) Within-Group Variance :-**\n",
        "\n",
        "- This component measures the variability of observations within each group. It reflects how much individual observations differ from their respective group means.\n",
        "\n",
        "- It is calculated by taking the squared differences between each observation and its group mean, summed across all groups.\n",
        "\n",
        "\n",
        "### **3) Total Variance :-**\n",
        "\n",
        "- The total variance in the dataset is the sum of both components :- (SS total = SSbetween + SSwithin)\n",
        "\n",
        "\n",
        "### **Contribution to the F-statistic :-**\n",
        "\n",
        "- The F-statistic in ANOVA is calculated as the ratio of the mean square of between-group variance to the mean square of within-group variance :- (F = MSbetween / MSwithin)\n",
        "\n",
        "- where :-\n",
        "- - MSbetween = SSbetween / g-1 (Here, g is the number of groups),\n",
        "- - MSwithin  = SSwithin / N-g (Here, where N is the total number of observations).\n",
        "\n",
        "\n",
        "### Interpretation of the F-statistic :-\n",
        "\n",
        "- The resulting F-statistic tests the null hypothesis that all group means are equal. A high F-value indicates that between-group variance is greater than within-group variance, suggesting that at least one group mean is significantly different from the others. This partitioning of variance thus serves as the foundation for assessing the significance of group differences in ANOVA.\n",
        "\n",
        "- A larger F-statistic indicates that a greater proportion of the total variance is due to differences between the groups rather than within the groups, suggesting that at least one group mean is significantly different from the others.\n",
        "\n",
        "- If the F-statistic is significantly larger than 1, it suggests that the between-group variance is greater than the within-group variance, leading to the rejection of the null hypothesis.\n",
        "\n",
        "- The partitioning of variance into between-group and within-group components provides a structured way to analyze the sources of variability in the data, which is essential for the calculation and interpretation of the F-statistic in ANOVA.\n"
      ],
      "metadata": {
        "id": "Ay1bqrlcPvKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7 :- Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing ?"
      ],
      "metadata": {
        "id": "eQcHkPc6PvHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### The classical (frequentist) approach to ANOVA with the Bayesian approach offer different perspectives on statistical analysis, particularly in how they handle uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "### Here are the key differences mentioned below :-\n",
        "\n",
        "\n",
        "\n",
        "### **Frequentist Approach :-**\n",
        "\n",
        "**1) Handling Uncertainty :-** Frequentists focus on long-run frequencies.\n",
        "\n",
        "- - Treats probability as the long-run frequency of events. Uncertainty is\n",
        "quantified through confidence intervals and p-values, which reflect the likelihood of observing the data under the null hypothesis.\n",
        "\n",
        "- - The results are often interpreted in terms of whether a certain threshold (e.g., p < 0.05) is met to make conclusions.\n",
        "\n",
        "**2) Parameter Estimation :-** Frequentists provide point estimates.\n",
        "\n",
        "- - Parameters are estimated using point estimates (e.g., sample means) and confidence intervals. The estimates are considered fixed, and the focus is on the sampling distribution of the estimator.\n",
        "\n",
        "- - It does not incorporate prior information about the parameters.\n",
        "\n",
        "**3) Hypothesis Testing :-** Frequentists use p-values and NHST.\n",
        "\n",
        "- - Uses null hypothesis significance testing (NHST), where hypotheses are tested using p-values. The focus is on rejecting or not rejecting the null hypothesis based on sample data.\n",
        "\n",
        "- - Confidence intervals can be used to provide an interval estimate for the parameter, but they do not directly test hypotheses.\n",
        "\n",
        "\n",
        "### **Bayesian Approach :-**\n",
        "\n",
        "**1) Handling Uncertainty :-** Bayesians incorporate prior beliefs into their models.\n",
        "\n",
        "- - Treats probability as a measure of belief or certainty about an event. Uncertainty is expressed using probability distributions.\n",
        "\n",
        "- - Provides a direct probabilistic interpretation of parameters, allowing for the incorporation of prior knowledge or beliefs about the parameters in the form of prior distributions.\n",
        "\n",
        "**2) Parameter Estimation :-** Bayesians provide full probability distributions.\n",
        "\n",
        "- - Parameters are treated as random variables with their own distributions (posterior distributions) that reflect both prior beliefs and the observed data.\n",
        "\n",
        "- - The estimates (posterior means, medians, or modes) are obtained through Bayes' theorem, which combines prior distributions with the likelihood of the observed data.\n",
        "\n",
        "**3) Hypothesis Testing :-** Bayesians use posterior probabilities and Bayes factors.\n",
        "\n",
        "- - Hypothesis testing is framed in terms of posterior probabilities. For example, one might compute the probability that one group mean is greater than another.\n",
        "\n",
        "- - Bayes factors can be used to compare the strength of evidence for one hypothesis against another, providing a more intuitive interpretation of the results.\n",
        "\n",
        "These differences lead to varying interpretations and applications of ANOVA results, with Bayesian methods offering a more flexible framework that can incorporate prior information and provide a richer understanding of uncertainty."
      ],
      "metadata": {
        "id": "SGERU5PjPvEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8 :- You have two sets of data representing the incomes of two different professions :-\n",
        "- 1) Profession A: [48, 52, 55, 60, 62]\n",
        "- 2) Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "Perform an F-test to determine if the variances of the two professions incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "**Task :-** Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "**Objective :-** Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n"
      ],
      "metadata": {
        "id": "C2PM07StYHVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### To perform an F-test to determine if the variances of the incomes of two different professions are equal, we have to use Python's statistical libraries like (Numpy, scipy.stats) and alpha is 0.05.\n",
        "\n",
        "\n",
        "### Conclusion of the test/code :-\n",
        "\n",
        "- If the p-value is less than 0.05, you would reject the null hypothesis, suggesting that the variances of incomes in Profession A and Profession B are significantly different.\n",
        "\n",
        "- If the p-value is greater than or equal to 0.05, you would fail to reject the null hypothesis, indicating no significant difference in variances.\n"
      ],
      "metadata": {
        "id": "N9DT3mo7PvBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries :-\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "CeuFGmthY5Eq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given Data for the two professions :-\n",
        "\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])"
      ],
      "metadata": {
        "id": "gAKiXHV4c0zn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform F-test (Levene's test for equal variances) :-\n",
        "\n",
        "f_statistic, p_value = stats.levene(profession_A, profession_B)\n",
        "f_statistic, p_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhy6lnBlc0Xo",
        "outputId": "f45bf95f-ce32-4a24-c3e7-f2cbca676c32"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7368421052631583, 0.4156507222081854)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display/print the results :-\n",
        "print(f\"F-statistic:- {f_statistic:.4f}\")\n",
        "print(f\"p-value:- {p_value:.4f}\")\n",
        "\n",
        "\n",
        "# Interpretation the result (determine whether to reject or fail to reject the null hypothesis based on the p-value) :-\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis:- The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis:- The variances are not significantly different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkOVlYUVY41H",
        "outputId": "7f844c57-7133-4207-b78c-98b75d05c79a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic:- 0.7368\n",
            "p-value:- 0.4157\n",
            "Fail to reject the null hypothesis:- The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9 :- Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data :-\n",
        "1) Region A :- [160, 162, 165, 158, 164]\n",
        "\n",
        "2) Region B :- [172, 175, 170, 168, 174]\n",
        "\n",
        "3) Region C :- [180, 182, 179, 185, 183]\n",
        "\n",
        "- **Task :-** Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "\n",
        "- **Objective :-** Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "VLnM_jxZPu-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "To perform a one-way ANOVA to test for statistically significant differences in average heights among three different regions, we can use Pythons statistical libraries like (Numpy, scipy.stats) and alpha is 0.05.\n",
        "\n",
        "\n",
        "### **Conclusion of the test :-**\n",
        "\n",
        "- If the p-value is less than 0.05, you would reject the null hypothesis, indicating that there are statistically significant differences in average heights among the regions.\n",
        "\n",
        "- If the p-value is greater than or equal to 0.05, you would fail to reject the null hypothesis, suggesting that there are no statistically significant differences."
      ],
      "metadata": {
        "id": "HfymGDLSPu63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries :-\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "eHtDWHfjbhsm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given data for the three regions :-\n",
        "\n",
        "region_A = np.array([160, 162, 165, 158, 164])\n",
        "region_B = np.array([172, 175, 170, 168, 174])\n",
        "region_C = np.array([180, 182, 179, 185, 183])"
      ],
      "metadata": {
        "id": "ipOpxkkUcmcF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-way ANOVA :-\n",
        "\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "f_statistic, p_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoNRByx_cgLt",
        "outputId": "156a9e41-9b4c-47cd-fe10-108d4288a76b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print/Display the results :-\n",
        "print(f\"F-statistic :- {f_statistic:.4f}\")\n",
        "print(f\"p-value :- {p_value:.4f}\")\n",
        "\n",
        "\n",
        "# Interpretation of the result (You will analyze the F-statistic and p-value to determine if there are significant differences among the groups.)\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis:- There are statistically significant differences in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis:- There are no statistically significant differences in average heights between regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb7dM1OwcsCb",
        "outputId": "317acd09-ce14-41ff-c134-7d9f34184660"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic :- 67.8733\n",
            "p-value :- 0.0000\n",
            "Reject the null hypothesis:- There are statistically significant differences in average heights between regions.\n"
          ]
        }
      ]
    }
  ]
}